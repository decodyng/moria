{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "import utils2; importlib.reload(utils2)\n",
    "from utils2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_block(inp, kernel_size): \n",
    "    num_outputs = inp.get_shape()[-1].value\n",
    "    h1 = Convolution2D(nb_filter=num_outputs, nb_row=kernel_size, nb_col=kernel_size, \n",
    "                      subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(220, 220, 3))\n",
    "#TODO: Mirror padding instead of zero-padding; use old module for that \n",
    "x = Convolution2D(nb_filter=32, nb_row=9, nb_col=9, subsample=1, border_mode='same')(x)\n",
    "x = Convolution2D(nb_filter=64, nb_row=3, nb_col=3, subsample=2, border_mode='same')(x)\n",
    "x = Convolution2D(nb_filter=128, nb_row=3, nb_col=3, subsample=2, border_mode='same')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_shape()[-1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: function to do conditional normalization. I think here it's just as simple as passing in a\n",
    "#an additional parameter when your'e training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "x = tf.Variable([1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_labels = tf.constant([[0, 1], [6, 7], [9, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3), Dimension(2)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [6, 7],\n",
       "       [9, 5]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_labels.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.nn.batch_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims_6:0' shape=(2, 1, 1, 3) dtype=int32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(tf.expand_dims(foo, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = tf.TensorShape([4]).concatenate(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(4), Dimension(6)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = slim.model_variable('beta2',\n",
    "                                shape=shape,\n",
    "                                dtype='float64',\n",
    "                                initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_labels = tf.constant([0, 0, 3, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(beta, style_labels).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = tf.TensorShape([num_categories]).concatenate(params_shape)\n",
    "var_collections = slim.utils.get_variable_collections(\n",
    "  variables_collections, name)\n",
    "var = slim.model_variable(name,\n",
    "                        shape=shape,\n",
    "                        dtype=dtype,\n",
    "                        initializer=initializer,\n",
    "                        collections=var_collections,\n",
    "                        trainable=trainable)\n",
    "conditioned_var = tf.gather(var, labels)\n",
    "conditioned_var = tf.expand_dims(tf.expand_dims(conditioned_var, 1), 1)\n",
    "return conditioned_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 3) dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = merge([inp for i in range(10)], mode='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(10, 3) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.reshape(x, shape=(10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Instance Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from ..engine import Layer, InputSpec\n",
    "from .. import initializers\n",
    "from .. import regularizers\n",
    "from .. import constraints\n",
    "from .. import backend as K\n",
    "\n",
    "\n",
    "class ConditionalInstanceNormalization(Layer):\n",
    "    \"\"\"Builds upon (i.e. borrows liberally from) the existing Batch \n",
    "    Normalization layer within Keras, but then implements Conditional Instance Normalization\n",
    "    as described here: https://arxiv.org/pdf/1610.07629.pdf\n",
    "    # Arguments\n",
    "        num_categories: The number of categories upon which you're conditionally normalizing. \n",
    "                        If using for style transfer, this corresponds to the\n",
    "                        number of distinct styles. Set to 2 by default. \n",
    "                        \n",
    "        axis: Integer, the axis that should be normalized\n",
    "            (typically the features axis).\n",
    "            For instance, after a `Conv2D` layer with\n",
    "            `data_format=\"channels_first\"`,\n",
    "            set `axis=1` in `BatchNormalization`.\n",
    "                    \n",
    "        momentum: Momentum for the moving average.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "            When the next layer is linear (also e.g. `nn.relu`),\n",
    "            this can be disabled since the scaling\n",
    "            will be done by the next layer.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        moving_mean_initializer: Initializer for the moving mean.\n",
    "        moving_variance_initializer: Initializer for the moving variance.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "TODO \n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_categories=2\n",
    "                 axis=-1,\n",
    "                 momentum=0.99,\n",
    "                 epsilon=1e-3,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 moving_mean_initializer='zeros',\n",
    "                 moving_variance_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(ConditionalInstanceNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.num_categories = num_categories \n",
    "        self.axis = axis\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.moving_mean_initializer = initializers.get(moving_mean_initializer)\n",
    "        self.moving_variance_initializer = initializers.get(moving_variance_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
    "                             'input tensor should have a defined dimension '\n",
    "                             'but the layer received an input with shape ' +\n",
    "                             str(input_shape) + '.')\n",
    "        self.input_spec = InputSpec(ndim=len(input_shape),\n",
    "                                    axes={self.axis: dim})\n",
    "        shape = (dim,)\n",
    "        \n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = [self.add_weight(shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint) for i in range(self.num_categories)]\n",
    "         \n",
    "        else:\n",
    "            self.gamma = None\n",
    "        if self.center:\n",
    "            self.beta = [self.add_weight(shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint) for i in range(self.num_categories)]\n",
    "\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.moving_mean = self.add_weight(\n",
    "            shape,\n",
    "            name='moving_mean',\n",
    "            initializer=self.moving_mean_initializer,\n",
    "            trainable=False)\n",
    "        self.moving_variance = self.add_weight(\n",
    "            shape,\n",
    "            name='moving_variance',\n",
    "            initializer=self.moving_variance_initializer,\n",
    "            trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        activations = inputs[0]\n",
    "        labels = inputs[1]\n",
    "        #so, in practice, I think we can keep a lot of this and \n",
    "        # just call it on inputs[0], keeping inputs[1] as a way to lookup \n",
    "        input_shape = K.int_shape(inputs[0])\n",
    "        \n",
    "        # Prepare broadcasting shape.\n",
    "        ndim = len(input_shape)\n",
    "        reduction_axes = list(range(len(input_shape)))\n",
    "        #[0, 1, 2, 3, 4]\n",
    "        del reduction_axes[self.axis]\n",
    "        #we delete the reduction axis along which we want to not marginalize \n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "        #broadcast shape will be [1] for every axis other than the feature axis\n",
    "\n",
    "        # Determines whether broadcasting is needed.\n",
    "        needs_broadcasting = (sorted(reduction_axes) != list(range(ndim))[:-1])\n",
    "        \n",
    "        normed, mean, variance = K.normalize_batch_in_training(\n",
    "            inputs[0], self.gamma, self.beta, reduction_axes,\n",
    "            epsilon=self.epsilon)\n",
    "        #okay, so the thing you want here is something with rows = len(batch), with each row \n",
    "        #being the oldschool equivalent of a gamma or beta \n",
    "\n",
    "        if training in {0, False}:\n",
    "            return normed\n",
    "        else:\n",
    "            self.add_update([K.moving_average_update(self.moving_mean,\n",
    "                                                     mean,\n",
    "                                                     self.momentum),\n",
    "                             K.moving_average_update(self.moving_variance,\n",
    "                                                     variance,\n",
    "                                                     self.momentum)],\n",
    "                            inputs)\n",
    "\n",
    "            def normalize_inference():\n",
    "                if needs_broadcasting:\n",
    "                    # In this case we must explictly broadcast all parameters.\n",
    "                    broadcast_moving_mean = K.reshape(self.moving_mean,\n",
    "                                                      broadcast_shape)\n",
    "                    broadcast_moving_variance = K.reshape(self.moving_variance,\n",
    "                                                          broadcast_shape)\n",
    "                    if self.center:\n",
    "                        broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "                    else:\n",
    "                        broadcast_beta = None\n",
    "                    if self.scale:\n",
    "                        broadcast_gamma = K.reshape(self.gamma,\n",
    "                                                    broadcast_shape)\n",
    "                    else:\n",
    "                        broadcast_gamma = None\n",
    "                    return K.batch_normalization(\n",
    "                        inputs,\n",
    "                        broadcast_moving_mean,\n",
    "                        broadcast_moving_variance,\n",
    "                        broadcast_beta,\n",
    "                        broadcast_gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "                else:\n",
    "                    return K.batch_normalization(\n",
    "                        inputs,\n",
    "                        self.moving_mean,\n",
    "                        self.moving_variance,\n",
    "                        self.beta,\n",
    "                        self.gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "\n",
    "        # Pick the normalized form corresponding to the training phase.\n",
    "        return K.in_train_phase(normed,\n",
    "                                normalize_inference,\n",
    "                                training=training)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'momentum': self.momentum,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'moving_mean_initializer': initializers.serialize(self.moving_mean_initializer),\n",
    "            'moving_variance_initializer': initializers.serialize(self.moving_variance_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(BatchNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
